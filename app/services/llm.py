"""
LLM module for interacting with Ollama.
"""

import subprocess
import time
import logging
from typing import Tuple, Optional

# Import configuration
from app.core.config import Config

logger = logging.getLogger(__name__)

def query_llm_with_ollama(
    prompt: str,
    model_name: str = None,
    temperature: float = None,
    top_k: int = None,
    top_p: float = None,
    max_tokens: Optional[int] = None
) -> str:
    """
    Query an LLM model via Ollama

    Args:
        prompt: The prompt to send to the model
        model_name: The model to use (defaults to config)
        temperature: Control randomization (0 = deterministic, 1 = creative)
        top_k: Limit token selection to k most probable tokens
        top_p: Select tokens with cumulative probability reaching p
        max_tokens: Maximum number of tokens to generate

    Returns:
        str: The response generated by the model
    """
    # Use provided parameters or defaults from config
    model_name = model_name or Config.get("LLM_MODEL")
    temperature = temperature if temperature is not None else float(Config.get("LLM_TEMPERATURE"))
    top_k = top_k if top_k is not None else int(Config.get("LLM_TOP_K"))
    top_p = top_p if top_p is not None else float(Config.get("LLM_TOP_P"))

    logger.info(f"Using LLM model: {model_name}")

    # Build the basic command for Ollama (without extra parameters for now)
    command = ["ollama", "run", model_name]

    try:
        logger.info(f"Querying Ollama with model: {model_name}")
        result = subprocess.run(
            command,
            input=prompt.encode('utf-8'),
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            check=True
        )

        response = result.stdout.decode('utf-8').strip()
        logger.info(f"Successfully received response from Ollama")
        return response

    except subprocess.CalledProcessError as e:
        error_message = e.stderr.decode('utf-8')
        logger.error(f"Error querying Ollama: {error_message}")
        # Return a formatted error message
        return f"Error: Unable to generate response. {error_message}"

    except Exception as e:
        logger.error(f"Unexpected error when querying Ollama: {str(e)}")
        return f"Error: An unexpected error occurred: {str(e)}"

def query_llm_directly(
    query: str,
    temperature: float = 0.7,
    max_tokens: Optional[int] = None
) -> Tuple[str, float]:
    """
    Query the LLM directly without additional context

    Args:
        query: The user query
        temperature: Model temperature control
        max_tokens: Maximum number of tokens to generate

    Returns:
        Tuple[str, float]: The response and processing time
    """
    start_time = time.time()

    prompt = f"Question: {query}\nAnswer:"
    response = query_llm_with_ollama(
        prompt=prompt,
        temperature=temperature,
        max_tokens=max_tokens
    )

    time_taken = time.time() - start_time
    logger.info(f"LLM direct query completed in {time_taken:.2f} seconds")

    return response, time_taken