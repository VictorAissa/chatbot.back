"""
LLM module for interacting with Ollama.
"""

import subprocess
import time
import logging
import json
import httpx
from typing import Tuple, Optional, AsyncGenerator, List, Dict, Any

from app.core.config import Config

logger = logging.getLogger(__name__)

def _get_llm_params(
        model_name: str = None,
        temperature: float = None,
        top_k: int = None,
        top_p: float = None
) -> Dict[str, Any]:
    return {
        "model_name": model_name or Config.get("LLM_MODEL"),
        "temperature": temperature if temperature is not None else float(Config.get("LLM_TEMPERATURE")),
        "top_k": top_k if top_k is not None else int(Config.get("LLM_TOP_K")),
        "top_p": top_p if top_p is not None else float(Config.get("LLM_TOP_P"))
    }

def query_llm_with_ollama(
    prompt: str,
    model_name: str = None,
    temperature: float = None,
    top_k: int = None,
    top_p: float = None,
    max_tokens: Optional[int] = None
) -> str:
    """
    Query an LLM model via Ollama

    Args:
        prompt: The prompt to send to the model
        model_name: The model to use (defaults to config)
        temperature: Control randomization (0 = deterministic, 1 = creative)
        top_k: Limit token selection to k most probable tokens
        top_p: Select tokens with cumulative probability reaching p
        max_tokens: Maximum number of tokens to generate

    Returns:
        str: The response generated by the model
    """
    params = _get_llm_params(model_name, temperature, top_k, top_p)
    model_name = params["model_name"]

    logger.info(f"Using LLM model: {model_name}")

    # Build the basic command for Ollama (without extra parameters for now)
    command = ["ollama", "run", model_name]

    try:
        logger.info(f"Querying Ollama with model: {model_name}")
        result = subprocess.run(
            command,
            input=prompt.encode('utf-8'),
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            check=True
        )

        response = result.stdout.decode('utf-8').strip()
        logger.info(f"Successfully received response from Ollama")
        return response

    except subprocess.CalledProcessError as e:
        error_message = e.stderr.decode('utf-8')
        logger.error(f"Error querying Ollama: {error_message}")
        # Return a formatted error message
        return f"Error: Unable to generate response. {error_message}"

    except Exception as e:
        logger.error(f"Unexpected error when querying Ollama: {str(e)}")
        return f"Error: An unexpected error occurred: {str(e)}"

def query_llm_directly(
    query: str,
    temperature: float = 0.7,
    max_tokens: Optional[int] = None
) -> Tuple[str, float]:
    """
    Query the LLM directly without additional context

    Args:
        query: The user query
        temperature: Model temperature control
        max_tokens: Maximum number of tokens to generate

    Returns:
        Tuple[str, float]: The response and processing time
    """
    start_time = time.time()

    prompt = f"Question: {query}\nAnswer:"
    response = query_llm_with_ollama(
        prompt=prompt,
        temperature=temperature,
        max_tokens=max_tokens
    )

    time_taken = time.time() - start_time
    logger.info(f"LLM direct query completed in {time_taken:.2f} seconds")

    return response, time_taken


async def query_llm_with_ollama_stream(
        prompt: str,
        model_name: str = None,
        temperature: float = None,
        top_k: int = None,
        top_p: float = None,
        max_tokens: Optional[int] = None
) -> AsyncGenerator[str, None]:
    """
    Stream response from an LLM model via Ollama API

    Args:
        prompt: The prompt to send to the model
        model_name: The model to use (defaults to config)
        temperature: Control randomization (0 = deterministic, 1 = creative)
        top_k: Limit token selection to k most probable tokens
        top_p: Select tokens with cumulative probability reaching p
        max_tokens: Maximum number of tokens to generate

    Yields:
        Tokens as they are generated
    """
    # Use provided parameters or defaults from config
    params = _get_llm_params(model_name, temperature, top_k, top_p)
    model_name = params["model_name"]
    temperature = params["temperature"]
    top_k = params["top_k"]
    top_p = params["top_p"]

    logger.info(f"Streaming from LLM model: {model_name}")

    ollama_api_base = Config.get("OLLAMA_API_BASE", "http://localhost:11434/api")

    request_data = {
        "model": model_name,
        "prompt": prompt,
        "stream": True,
        "options": {
            "temperature": temperature,
            "top_k": top_k,
            "top_p": top_p
        }
    }

    if max_tokens is not None:
        request_data["options"]["num_predict"] = max_tokens

    try:
        async with httpx.AsyncClient(timeout=60.0) as client:
            async with client.stream("POST", f"{ollama_api_base}/generate", json=request_data) as response:
                if response.status_code != 200:
                    error_text = await response.aread()
                    logger.error(f"Error from Ollama API: {error_text}")
                    yield f"Error: {error_text}"
                    return

                buffer = b""
                async for chunk in response.aiter_bytes():
                    buffer += chunk

                    while b"\n" in buffer:
                        line, buffer = buffer.split(b"\n", 1)
                        if line:
                            try:
                                response_json = json.loads(line)
                                if "response" in response_json:
                                    yield response_json["response"]
                            except json.JSONDecodeError:
                                logger.warning(f"Failed to parse JSON: {line}")

    except Exception as e:
        logger.error(f"Error in streaming LLM query: {str(e)}")
        yield f"Error communicating with LLM: {str(e)}"


async def query_llm_stream(
        query: str,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None
) -> AsyncGenerator[str, None]:
    """
    Stream response from the LLM directly without additional context

    Args:
        query: The user query
        temperature: Model temperature control
        max_tokens: Maximum number of tokens to generate

    Yields:
        Tokens as they are generated
    """
    prompt = f"Question: {query}\nAnswer:"

    async for token in query_llm_with_ollama_stream(
            prompt=prompt,
            temperature=temperature,
            max_tokens=max_tokens
    ):
        yield token


async def rag_pipeline_stream(
        query: str,
        docs: List[Dict[str, Any]],
        temperature: float = 0.7,
        max_tokens: Optional[int] = None
) -> AsyncGenerator[str, None]:
    """
    Stream response from the RAG pipeline

    Args:
        query: The user query
        docs: Retrieved documents for context
        temperature: Model temperature control
        max_tokens: Maximum number of tokens to generate

    Yields:
        Tokens as they are generated
    """
    context = "\n\n".join([doc["text"] for doc in docs])

    prompt = f"Context:\n{context}\n\nQuestion: {query}\n\nAnswer:"

    async for token in query_llm_with_ollama_stream(
            prompt=prompt,
            temperature=temperature,
            max_tokens=max_tokens
    ):
        yield token