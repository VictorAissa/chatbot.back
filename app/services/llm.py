"""
LLM module for interacting with Ollama.
"""

import subprocess
import time
import logging
import json
import httpx
from typing import Tuple, Optional, AsyncGenerator, Dict, Any

from app.core.config import Config

logger = logging.getLogger(__name__)
ollama_api_base = Config.get("OLLAMA_API_BASE", "http://localhost:11434/api")

def _get_llm_params(
        model_name: str = None,
        temperature: float = None,
        top_k: int = None,
        top_p: float = None
) -> Dict[str, Any]:
    return {
        "model_name": model_name or Config.get("LLM_MODEL"),
        "temperature": temperature if temperature is not None else float(Config.get("LLM_TEMPERATURE")),
        "top_k": top_k if top_k is not None else int(Config.get("LLM_TOP_K")),
        "top_p": top_p if top_p is not None else float(Config.get("LLM_TOP_P"))
    }

def query_llm_with_ollama_api(
    prompt: str,
    model_name: str = None,
    temperature: float = None,
    top_k: int = None,
    top_p: float = None,
    max_tokens: Optional[int] = None
) -> str:
    """
    Query an LLM model via Ollama

    Args:
        prompt: The prompt to send to the model
        model_name: The model to use (defaults to config)
        temperature: Control randomization (0 = deterministic, 1 = creative)
        top_k: Limit token selection to k most probable tokens
        top_p: Select tokens with cumulative probability reaching p
        max_tokens: Maximum number of tokens to generate

    Returns:
        str: The response generated by the model
    """
    params = _get_llm_params(model_name, temperature, top_k, top_p)
    model_name = params["model_name"]
    temperature = params["temperature"]
    top_k = params["top_k"]
    top_p = params["top_p"]

    request_data = {
        "model": model_name,
        "prompt": prompt,
        "stream": False,
        "options": {
            "temperature": temperature,
            "top_k": top_k,
            "top_p": top_p
        }
    }

    if max_tokens is not None:
        request_data["options"]["num_predict"] = max_tokens

    try:
        response = httpx.post(f"{ollama_api_base}/generate", json=request_data, timeout=120.0)

        response.raise_for_status()
        result = response.json()
        return result.get("response", "")
    except Exception as e:
        logger.error(f"Error querying Ollama API: {str(e)}")
        return f"Error: {str(e)}"


def qquery_llm_with_ollama_subprocess(
        prompt: str,
        model_name: str = None,
        max_tokens: Optional[int] = None
) -> str:
    """
    Query an LLM model via Ollama using subprocess (fast but no temperature control)
    """
    params = _get_llm_params(model_name, None, None, None)
    model_name = params["model_name"]

    logger.info(f"Using LLM model via subprocess: {model_name}")

    # Basic command for Ollama without temperature parameters
    command = ["ollama", "run", model_name]

    try:
        logger.info(f"Querying Ollama with model: {model_name}")
        result = subprocess.run(
            command,
            input=prompt.encode('utf-8'),
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            check=True,
            timeout=120
        )

        response = result.stdout.decode('utf-8').strip()
        logger.info(f"Successfully received response from Ollama (direct method)")
        return response

    except subprocess.TimeoutExpired:
        logger.error("Subprocess timed out after 60 seconds")
        return "Error: Subprocess timed out. The model is taking too long to respond."
    except subprocess.CalledProcessError as e:
        error_message = e.stderr.decode('utf-8')
        logger.error(f"Error querying Ollama: {error_message}")
        return f"Error: Unable to generate response. {error_message}"
    except Exception as e:
        logger.error(f"Unexpected error when querying Ollama: {str(e)}")
        return f"Error: An unexpected error occurred: {str(e)}"

def query_llm_with_fallback(
        prompt: str,
        model_name: str = None,
        temperature: float = None,
        top_k: int = None,
        top_p: float = None,
        max_tokens: Optional[int] = None
) -> str:
    """
    Query LLM with temperature control if possible, falling back to direct method if timeout occurs
    """
    # If no temperature is specified, use the direct method immediately
    if temperature is None :
        logger.info("No specific temperature requested, using direct method")
        return qquery_llm_with_ollama_subprocess(prompt, model_name, max_tokens)

    # Try with API first to get temperature control
    try:
        logger.info(f"Attempting API request with temperature={temperature}")

        result = query_llm_with_ollama_api(prompt, model_name, temperature, top_k, top_p, max_tokens)
        return result

    except (httpx.TimeoutException, httpx.ReadTimeout):
        logger.warning("API request timed out, falling back to subprocess method")
        return qquery_llm_with_ollama_subprocess(prompt, model_name, max_tokens)
    except Exception as e:
        logger.error(f"API request failed: {str(e)}, falling back to subprocess method")
        return qquery_llm_with_ollama_subprocess(prompt, model_name, max_tokens)


async def query_llm_with_ollama_stream(
        prompt: str,
        model_name: str = None,
        temperature: float = None,
        top_k: int = None,
        top_p: float = None,
        max_tokens: Optional[int] = None
) -> AsyncGenerator[str, None]:
    """
    Stream response from an LLM model via Ollama API

    Args:
        prompt: The prompt to send to the model
        model_name: The model to use (defaults to config)
        temperature: Control randomization (0 = deterministic, 1 = creative)
        top_k: Limit token selection to k most probable tokens
        top_p: Select tokens with cumulative probability reaching p
        max_tokens: Maximum number of tokens to generate

    Yields:
        Tokens as they are generated
    """
    # Use provided parameters or defaults from config
    params = _get_llm_params(model_name, temperature, top_k, top_p)
    model_name = params["model_name"]
    temperature = params["temperature"]
    top_k = params["top_k"]
    top_p = params["top_p"]

    logger.info(f"Streaming from LLM model: {model_name}")

    request_data = {
        "model": model_name,
        "prompt": prompt,
        "stream": True,
        "options": {
            "temperature": temperature,
            "top_k": top_k,
            "top_p": top_p
        }
    }

    if max_tokens is not None:
        request_data["options"]["num_predict"] = max_tokens

    try:
        async with httpx.AsyncClient(timeout=60.0) as client:
            async with client.stream("POST", f"{ollama_api_base}/generate", json=request_data) as response:
                if response.status_code != 200:
                    error_text = await response.aread()
                    logger.error(f"Error from Ollama API: {error_text}")
                    yield f"Error: {error_text}"
                    return

                buffer = b""
                async for chunk in response.aiter_bytes():
                    buffer += chunk

                    while b"\n" in buffer:
                        line, buffer = buffer.split(b"\n", 1)
                        if line:
                            try:
                                response_json = json.loads(line)
                                if "response" in response_json:
                                    yield response_json["response"]
                            except json.JSONDecodeError:
                                logger.warning(f"Failed to parse JSON: {line}")

    except Exception as e:
        logger.error(f"Error in streaming LLM query: {str(e)}")
        yield f"Error communicating with LLM: {str(e)}"


async def query_llm_stream(
        query: str,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None
) -> AsyncGenerator[str, None]:
    """
    Stream response from the LLM directly without additional context

    Args:
        query: The user query
        temperature: Model temperature control
        max_tokens: Maximum number of tokens to generate

    Yields:
        Tokens as they are generated
    """
    prompt = f"Question: {query}\nAnswer:"

    async for token in query_llm_with_ollama_stream(
            prompt=prompt,
            temperature=temperature,
            max_tokens=max_tokens
    ):
        yield token
